{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#Basic Functions\nimport numpy as np \nimport pandas as pd \nfrom collections import Counter\n\n#Undersampling Techniques\nfrom imblearn.under_sampling import NeighbourhoodCleaningRule\nfrom imblearn.under_sampling import NearMiss \n\n#Tree Printing \nfrom pprint import pprint","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-13T05:34:42.481719Z","iopub.execute_input":"2021-06-13T05:34:42.482124Z","iopub.status.idle":"2021-06-13T05:34:42.488001Z","shell.execute_reply.started":"2021-06-13T05:34:42.482089Z","shell.execute_reply":"2021-06-13T05:34:42.486726Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"#Pull out the raw data\ndef fetchRawDataset() : \n    dataset = pd.read_csv('../input/restaurant-model-dataset/Dataset.csv')\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2021-06-13T05:34:42.489453Z","iopub.execute_input":"2021-06-13T05:34:42.489709Z","iopub.status.idle":"2021-06-13T05:34:42.508821Z","shell.execute_reply.started":"2021-06-13T05:34:42.489684Z","shell.execute_reply":"2021-06-13T05:34:42.507526Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"#Pick out the features\n#Features: [\"is_open\", \"stars\", \"American\", \"Mexican\", \"Italian\", \"Japanese\", \"Chinese\", \"Thai\", \"Mediterranean\", \"French\",\"Vietnamese\", \"Greek\", \"Indian\", \"Korean\", \"Hawaiian\", \"African\", \"Spanish\", \"Middle Eastern\", \"Other\", \"age\", \"review_count_16_19\", \"review_normalized\", \"pos\", \"neg\", \"neutral\", \"pos_norm\", \"neg_norm\", \"neutral_norm\", \"bi_goodfood_text\", \"bi_goodloc_text\", \"bi_bad_text\", \"Delivery\", \"RestaurantsPriceRange2\", \"OutdoorSeating\", \"HasTV\", \"Touristy\", \"Hipster\", \"Romantic\", \"Divey\", \"Intimate\", \"Trendy\", \"Upscale\", \"Classy\", \"Casual\", \"Alc_None_served\", \"Alc_beer_and_wine\", \"Alc_full_bar\", \"open_breakfast\", \"open_lunch\", \"open_dinner\", \"open_hours_meals\", \"displayed_open_hours\", \"is_crowded\", \"is_chain\", \"operation_years\"]\ndef featureSelection(dataset) : \n    #Shorter Version: Ratings, Restaurant Category, Open Hours, Crowded, Chain\n    chosen_features = [\"stars\", \"American\", \"Mexican\", \"Italian\", \"Japanese\", \"Chinese\", \"Thai\", \"Mediterranean\", \"French\",\"Vietnamese\", \"Greek\", \"Indian\", \"Korean\", \"Hawaiian\", \"African\", \"Spanish\", \"Middle Eastern\", \"Other\", \"open_breakfast\", \"open_lunch\", \"open_dinner\", \"is_crowded\", \"is_chain\", \"is_open\"]\n    return dataset[chosen_features]","metadata":{"execution":{"iopub.status.busy":"2021-06-13T05:34:42.510752Z","iopub.execute_input":"2021-06-13T05:34:42.511076Z","iopub.status.idle":"2021-06-13T05:34:42.521040Z","shell.execute_reply.started":"2021-06-13T05:34:42.511047Z","shell.execute_reply":"2021-06-13T05:34:42.520172Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"#Set feature types\ndef assignTypes(dataset, newDataset = pd.DataFrame()) : \n    for feature in dataset.columns : \n        if dataset[feature].nunique() <= 4 : \n            newDataset[feature] = dataset[feature].astype(\"category\").copy()\n        else : \n            newDataset[feature] = dataset[feature]\n    return newDataset","metadata":{"execution":{"iopub.status.busy":"2021-06-13T05:34:42.522237Z","iopub.execute_input":"2021-06-13T05:34:42.522657Z","iopub.status.idle":"2021-06-13T05:34:42.534814Z","shell.execute_reply.started":"2021-06-13T05:34:42.522622Z","shell.execute_reply":"2021-06-13T05:34:42.534129Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"#Flip the target variable\ndef flipClass(dataset) :\n    new_mappings = { 0 : 1, 1 : 0 }\n    dataset[\"is_open\"].replace(new_mappings, inplace=True)\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2021-06-13T05:34:42.536214Z","iopub.execute_input":"2021-06-13T05:34:42.536568Z","iopub.status.idle":"2021-06-13T05:34:42.546776Z","shell.execute_reply.started":"2021-06-13T05:34:42.536542Z","shell.execute_reply":"2021-06-13T05:34:42.546046Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"#Balancethe dataset\ndef NCRSampling(dataset) : \n    undersample = NeighbourhoodCleaningRule(n_neighbors = 3, threshold_cleaning = 0.1)\n    features, labels = undersample.fit_resample(dataset.drop([\"is_open\"], axis = 1), dataset[\"is_open\"])\n    features[\"is_open\"] = labels\n    return features\n\ndef NearMissSampling(dataset) :\n    undersample = NearMiss(version=3, n_neighbors_ver3=3)\n    features, labels = undersample.fit_resample(dataset.drop([\"is_open\"], axis = 1), dataset[\"is_open\"]) \n    features[\"is_open\"] = labels\n    return features","metadata":{"execution":{"iopub.status.busy":"2021-06-13T05:34:42.548278Z","iopub.execute_input":"2021-06-13T05:34:42.548693Z","iopub.status.idle":"2021-06-13T05:34:42.562343Z","shell.execute_reply.started":"2021-06-13T05:34:42.548656Z","shell.execute_reply":"2021-06-13T05:34:42.561596Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"#Preprocess the dataset\ndef fetchDataset() : \n    dataset = fetchRawDataset() #Get initial dataset\n    dataset = featureSelection(dataset) #Feature selection\n    dataset = assignTypes(dataset) #Type allocation\n    dataset = flipClass(dataset) #Set target variable\n    \n    imbalanced_dataset = NCRSampling(dataset.copy())\n    balanced_dataset = NearMissSampling(dataset.copy())\n    \n    return dataset, imbalanced_dataset, balanced_dataset","metadata":{"execution":{"iopub.status.busy":"2021-06-13T05:34:42.563330Z","iopub.execute_input":"2021-06-13T05:34:42.563673Z","iopub.status.idle":"2021-06-13T05:34:42.573950Z","shell.execute_reply.started":"2021-06-13T05:34:42.563648Z","shell.execute_reply":"2021-06-13T05:34:42.573315Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"#Fetch Model Dataset\ndebug_dataset, imbalanced_dataset, balanced_dataset = fetchDataset()\nprint(\"Debugging Set  : \", Counter(debug_dataset[\"is_open\"]))\nprint(\"Unbalanced Set : \", Counter(imbalanced_dataset[\"is_open\"]))\nprint(\"Balanced Set   : \", Counter(balanced_dataset[\"is_open\"]))","metadata":{"execution":{"iopub.status.busy":"2021-06-13T05:34:42.574993Z","iopub.execute_input":"2021-06-13T05:34:42.575379Z","iopub.status.idle":"2021-06-13T05:34:43.067204Z","shell.execute_reply.started":"2021-06-13T05:34:42.575344Z","shell.execute_reply":"2021-06-13T05:34:43.066262Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Debugging Set  :  Counter({0: 2301, 1: 582})\nUnbalanced Set :  Counter({0: 1501, 1: 582})\nBalanced Set   :  Counter({0: 582, 1: 582})\n","output_type":"stream"}]},{"cell_type":"code","source":"#Debugging dataset\nfrom sklearn.model_selection import train_test_split\n\ndef dataset_split(dataset, test_size = 0.2) :\n    X_train, X_test = train_test_split(dataset, test_size = test_size)\n    return X_train.reset_index(drop = True), X_test.reset_index(drop = True)\n\n#For Debugging\ndebug_training_fold, debug_testing_fold = dataset_split(debug_dataset)\nprint(\"Training Shape:\", debug_training_fold.shape, \"\\tTesting Shape:\", debug_testing_fold.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-13T05:34:43.068995Z","iopub.execute_input":"2021-06-13T05:34:43.069796Z","iopub.status.idle":"2021-06-13T05:34:43.090173Z","shell.execute_reply.started":"2021-06-13T05:34:43.069745Z","shell.execute_reply":"2021-06-13T05:34:43.089177Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Training Shape: (2306, 25) \tTesting Shape: (577, 25)\n","output_type":"stream"}]},{"cell_type":"code","source":"#Entropy - Splitting criterion\ndef entropy(feature, debugging = 0) : \n\n    entropy = 0\n    values, counts = np.unique(feature, return_counts = True) #Get unique feature value and their counts    \n    \n    for i in range(len(values)) : \n        entropy += - (counts[i]/np.sum(counts)) * (np.log2(counts[i]/np.sum(counts))) #Entropy formula\n    \n    if debugging == 1 : \n        print(\"Number of Unique feature values: \", len(values))\n        for i, j in zip(values, counts) : \n            print(i, \" ==> \", j)\n        print(\"Entropy: \", entropy)\n        \n    return entropy\n\n#entropy(debug_training_fold[\"stars\"], 1) #For Debugging","metadata":{"execution":{"iopub.status.busy":"2021-06-13T05:34:43.093394Z","iopub.execute_input":"2021-06-13T05:34:43.094127Z","iopub.status.idle":"2021-06-13T05:34:43.102744Z","shell.execute_reply.started":"2021-06-13T05:34:43.094085Z","shell.execute_reply":"2021-06-13T05:34:43.101770Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"#Information Gain - Category\ndef Information_Gain(current_dataset, feature_name, target_variable = \"is_open\", debugging = 0) : \n    \n    if debugging == 1 :\n        print(\"\\nParent Entropy: \")\n    entropy_dataset = entropy(current_dataset[target_variable], debugging) #Dataset entropy\n    \n    #Feature Weighted entropy\n    entropy_feature = 0\n    values, counts = np.unique(current_dataset[feature_name], return_counts = True) #Get unique feature values and their counts \n    \n    for i in range(len(values)) : \n        if debugging == 1 : \n            print(\"\\nFeature Name: \", feature_name, \"\\tValue: \", values[i], \"\\tCount: \", counts[i])\n        entropy_feature_value = entropy(current_dataset.where(current_dataset[feature_name] == values[i]).dropna()[target_variable], debugging)\n        entropy_feature += counts[i] / np.sum(counts) * entropy_feature_value #Entropy becomes weighted based on the count of a particular value\n    \n    Information_Gain = entropy_dataset - entropy_feature\n    if debugging == 1 : \n        print(\"\\nParent Entropy: \", entropy_dataset, \"\\tFeature '\", feature_name, \"' Weighted Entropy: \", entropy_feature)\n        print(\"Information Gain: \", Information_Gain)\n        \n    return Information_Gain\n\n#Information_Gain(debug_training_fold, \"American\", debugging = 1) #For Debugging","metadata":{"execution":{"iopub.status.busy":"2021-06-13T05:34:43.105037Z","iopub.execute_input":"2021-06-13T05:34:43.105688Z","iopub.status.idle":"2021-06-13T05:34:43.121366Z","shell.execute_reply.started":"2021-06-13T05:34:43.105647Z","shell.execute_reply":"2021-06-13T05:34:43.120205Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"#Information Gain - Continuous\ndef Count_Information_Gain(current_dataset, feature_name, target_variable = \"is_open\", debugging = 0, threshold = 0) : \n    \n    values = sorted(current_dataset[feature_name].unique()) #Checking each value for best binary split\n    \n    best_IG = best_split = 0\n\n    if debugging == 1 :\n        print(\"\\nContinuous feature found:\")\n    \n    for i in values : #Visualizing the dataset as categorical w.r.t each value <>\n        idx = current_dataset[current_dataset[feature_name] <= i].index\n        temp_dataset = current_dataset.copy()\n        temp_dataset[feature_name] = \">\" + str(i)\n        temp_dataset.loc[idx, feature_name] = \"<=\" + str(i)        \n                \n        Info_Gain = Information_Gain(temp_dataset, feature_name, target_variable, debugging)\n        if Info_Gain >= best_IG : \n            best_split = i\n            best_IG = Info_Gain \n            best_dataset = temp_dataset.copy()\n        \n    if debugging == 1 and threshold == 0 :  \n        print(\"\\nBest Split Point: \", best_split)\n\n    if threshold == 1 : #To get the best split point\n        return best_split \n    \n    return best_IG, best_dataset #Information Gain & Categorical Feature\n\n#Count_Information_Gain(debug_training_fold, \"stars\", debugging = 1) #For Debugging","metadata":{"execution":{"iopub.status.busy":"2021-06-13T05:34:43.123078Z","iopub.execute_input":"2021-06-13T05:34:43.123770Z","iopub.status.idle":"2021-06-13T05:34:43.135313Z","shell.execute_reply.started":"2021-06-13T05:34:43.123724Z","shell.execute_reply":"2021-06-13T05:34:43.134186Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"#Gain Ratio\ndef Gain_Ratio(current_dataset, feature_name, target_variable = \"is_open\", debugging = 0) : \n    \n    if current_dataset[feature_name].dtype.name != \"category\" : #For Continuous values\n        Info_Gain, temp_dataset = Count_Information_Gain(current_dataset, feature_name, target_variable, debugging)        \n        Split_Info = entropy(temp_dataset[feature_name], debugging)\n    \n    else : #For Categorical values\n        Info_Gain = Information_Gain(current_dataset, feature_name, target_variable, debugging)\n        Split_Info = entropy(current_dataset[feature_name], debugging)\n    \n    Gain_Ratio = Info_Gain / (Split_Info + 0.1) #To face the zero entropy problem\n    \n    if debugging == 1 : \n        print(\"\\nInformation Gain: \", Info_Gain, \"\\tFeature '\", feature_name, \"' Split Info (Entropy): \", Split_Info)\n        print(\"Gain Ratio: \", Gain_Ratio)\n\n    return Gain_Ratio\n\n#Gain_Ratio(debug_training_fold, \"American\", debugging = 1) #For Debugging","metadata":{"execution":{"iopub.status.busy":"2021-06-13T05:34:43.137056Z","iopub.execute_input":"2021-06-13T05:34:43.137697Z","iopub.status.idle":"2021-06-13T05:34:43.152959Z","shell.execute_reply.started":"2021-06-13T05:34:43.137653Z","shell.execute_reply":"2021-06-13T05:34:43.152213Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"#DTree Construction\ndef C4_5(current_dataset, orginal_dataset, features, target_variable = \"is_open\", parent_node_label = None, debugging = 0, max_height = 10) :\n    \n    if len(np.unique(current_dataset[target_variable])) == 1  : #If the target_varible is pure return the label\n        return np.unique(current_dataset[target_variable])[0]\n    \n    elif len(current_dataset) == 0 : #If dataset is empty, return the mode target feature value in the original dataset\n        return np.unique(orginal_dataset[target_variable])[np.argmax(np.unique(orginal_dataset[target_variable], return_counts = True)[1])]\n    \n    elif len(features) == 0 : #If feature list is empty return the label\n        return parent_node_label\n    \n    elif max_height == -1 : \n        return np.unique(current_dataset[target_variable])[np.argmax(np.unique(current_dataset[target_variable], return_counts = True)[1])]\n    \n    else: #Tree Construction\n        #Set the default value for this node --> The mode target feature value of the current node\n        parent_node_label = np.unique(current_dataset[target_variable])[np.argmax(np.unique(current_dataset[target_variable], return_counts = True)[1])]\n\n        #Choosing the feature that gives the best split\n        GR_values = []\n        for feature in features : \n            GR_values.append(Gain_Ratio(current_dataset,feature))\n        best_feature = features[np.argmax(GR_values)]\n\n        #Choosing the best feature as the root node\n        tree = { best_feature : { }, \"gain_ratio\" : max(GR_values), \"sample_count\" : len(current_dataset.index), \"values\" : [], \"class\" : parent_node_label, \"confidence\" : { }, \"leaf_values\" : {} }\n        \n        pos_count = len(current_dataset[current_dataset[\"is_open\"] == 1].index)\n        neg_count = len(current_dataset[current_dataset[\"is_open\"] == 0].index)\n        tree[\"values\"] = [pos_count, neg_count]\n        \n        if current_dataset[best_feature].dtype.name != \"category\" : \n                       \n            split_point = Count_Information_Gain(current_dataset, best_feature, target_variable, debugging, threshold = 1)\n                        \n            sub_dataset = current_dataset.where(current_dataset[best_feature] <= split_point).dropna() #Choosing the points belonging to feature value\n            sub_tree = C4_5(sub_dataset, orginal_dataset, features, target_variable, parent_node_label, debugging, max_height - 1) #Recursive construction\n            tree[best_feature][\"<=\" + str(split_point)] = sub_tree  #Add the sub tree, grown from the sub_dataset to the tree under the root node\n            if type(sub_tree) is not dict :\n                if len(sub_dataset.index) == 0 : \n                    positive_class_prob = len(current_dataset.loc[current_dataset[target_variable] == 1].index) / len(current_dataset.index) \n                else : \n                    positive_class_prob = len(sub_dataset.loc[sub_dataset[target_variable] == 1].index) / len(sub_dataset.index)  \n                tree[\"confidence\"][\"<=\" + str(split_point)] = positive_class_prob\n                pos_count = len(sub_dataset[sub_dataset[\"is_open\"] == 1].index)\n                neg_count = len(sub_dataset[sub_dataset[\"is_open\"] == 0].index)\n                tree[\"leaf_values\"][\"<=\" + str(split_point)] = [pos_count, neg_count] \n            \n            sub_dataset = current_dataset.where(current_dataset[best_feature] > split_point).dropna() #Choosing the points belonging to feature value\n            sub_tree = C4_5(sub_dataset, orginal_dataset, features, target_variable, parent_node_label, debugging, max_height - 1) #Recursive construction\n            tree[best_feature][\">\" + str(split_point)] = sub_tree  #Add the sub tree, grown from the sub_dataset to the tree under the root node\n            if type(sub_tree) is not dict :\n                if len(sub_dataset.index) == 0 : \n                    positive_class_prob = len(current_dataset.loc[current_dataset[target_variable] == 1].index) / len(current_dataset.index) \n                else : \n                    positive_class_prob = len(sub_dataset.loc[sub_dataset[target_variable] == 1].index) / len(sub_dataset.index)  \n                tree[\"confidence\"][\">\" + str(split_point)] = positive_class_prob\n                pos_count = len(sub_dataset[sub_dataset[\"is_open\"] == 1].index)\n                neg_count = len(sub_dataset[sub_dataset[\"is_open\"] == 0].index)\n                tree[\"leaf_values\"][\">\" + str(split_point)] = [pos_count, neg_count] \n            \n        else : \n            \n            #Removing the best feature from the list\n            features = [ i for i in features if i != best_feature ]\n\n            #Branching for each feature value\n            for value in np.unique(current_dataset[best_feature]) :\n                sub_dataset = current_dataset.where(current_dataset[best_feature] == value).dropna() #Choosing the points belonging to feature value\n                sub_tree = C4_5(sub_dataset, orginal_dataset, features, target_variable, parent_node_label, debugging, max_height - 1) #Recursive construction  \n                tree[best_feature][value] = sub_tree  #Add the sub tree, grown from the sub_dataset to the tree under the root node\n                if type(sub_tree) is not dict:\n                    if len(sub_dataset.index) == 0 : \n                        positive_class_prob = len(current_dataset.loc[current_dataset[target_variable] == 1].index) / len(current_dataset.index) \n                    else : \n                        positive_class_prob = len(sub_dataset.loc[sub_dataset[target_variable] == 1].index) / len(sub_dataset.index)  \n                    tree[\"confidence\"][value] = positive_class_prob \n                    pos_count = len(sub_dataset[sub_dataset[\"is_open\"] == 1].index)\n                    neg_count = len(sub_dataset[sub_dataset[\"is_open\"] == 0].index)\n                    tree[\"leaf_values\"][value] = [pos_count, neg_count] \n        return tree  \n\nDebug_Tree = C4_5(debug_training_fold, debug_training_fold, debug_training_fold.columns[:-1], max_height = 10) #For Debugging\n#pprint(Debug_Tree)","metadata":{"execution":{"iopub.status.busy":"2021-06-13T05:34:43.154555Z","iopub.execute_input":"2021-06-13T05:34:43.155231Z","iopub.status.idle":"2021-06-13T05:43:24.657017Z","shell.execute_reply.started":"2021-06-13T05:34:43.155191Z","shell.execute_reply":"2021-06-13T05:43:24.656003Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"#DTree Traversal\ndef Predict(datapoint, decision_tree, getProb = 0, debugging = 0):\n    \n    probability = 0 \n    continuous_features = [\"stars\", \"review_count_16_19\", \"pos\", \"neg\", \"neutral\", \"bi_goodfood_text\", \"bi_goodloc_text\", \"bi_bad_text\"] #Continuous Features\n\n    for key in list(datapoint.keys()):\n        \n        if key in list(decision_tree.keys()):\n            \n            if key in continuous_features : #Continuous Feature\n                                \n                if list(decision_tree[key].keys())[0][0] == \"<\" : \n                    threshold = float(list(decision_tree[key].keys())[0][2:])\n                else : \n                    threshold = float(list(decision_tree[key].keys())[0][1:])\n                \n                if datapoint[key] <= threshold : \n                    sub_tree = decision_tree[key][\"<=\" + str(threshold)]\n                    if type(sub_tree) is not dict : \n                        if getProb : probability = decision_tree[\"confidence\"][\"<=\" + str(threshold)]\n                    \n                    if debugging == 1 : \n                        print(\"\\nContinuous Feature: \", key, \"\\tSplit Threshold: <=\", threshold)\n                        print(\"\\nResult after Traversal: \", sub_tree)\n                else : \n                    sub_tree = decision_tree[key][\">\" + str(threshold)]\n                    if type(sub_tree) is not dict : \n                        if getProb : probability = decision_tree[\"confidence\"][\">\" + str(threshold)]\n                    \n                    if debugging == 1 : \n                        print(\"\\nContinuous Feature: \", key, \"\\tSplit Threshold: >\", threshold)\n                        print(\"\\nResult after Traversal: \", sub_tree)\n                            \n            else : #Categorical Feature\n                sub_tree = decision_tree[key][datapoint[key]]\n                if type(sub_tree) is not dict : \n                    if getProb : probability = decision_tree[\"confidence\"][datapoint[key]] \n                \n                if debugging == 1 : \n                    print(\"\\nCategorical Feature: \", key, \"\\tSplit Value: \", datapoint[key])\n                    print(\"\\nResult after Traversal: \", sub_tree)\n                    \n            if isinstance(sub_tree, dict):\n                return Predict(datapoint, sub_tree, getProb, debugging) #If subtree is dictionary, recursive call\n            \n            else: #If subtree is a leaf, return label\n                if getProb == 1 : \n                    return probability\n                return sub_tree\n            \n#debug_query = {'stars': 4.5, 'American': 1, 'Mexican': 0, 'Italian': 0, 'Japanese': 0, 'Chinese': 0, 'Thai': 0, 'Mediterranean': 0, 'French': 0, 'Vietnamese': 0, 'Greek': 0, 'Indian': 0, 'Korean': 0, 'Hawaiian': 0, 'African': 0, 'Spanish': 0, 'Middle Eastern': 0, 'Other': 0, 'open_breakfast': 0, 'open_lunch': 1, 'open_dinner': 0, 'is_crowded': 0, 'is_chain': 0}\n#Predict(debug_query, Debug_Tree, getProb = 1, debugging = 1) #Debugging","metadata":{"execution":{"iopub.status.busy":"2021-06-13T05:43:24.658454Z","iopub.execute_input":"2021-06-13T05:43:24.658776Z","iopub.status.idle":"2021-06-13T05:43:24.671730Z","shell.execute_reply.started":"2021-06-13T05:43:24.658746Z","shell.execute_reply":"2021-06-13T05:43:24.670754Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"#Testing classifier\ndef Validation(testing_values, decision_tree, debugging = 0):\n    \n    datapoints = testing_values.iloc[:,:-1].to_dict(orient = \"records\") #Converting each datapoint into a dict\n    predicted = pd.DataFrame(columns=[\"pred_value\", \"pred_probability\"]) #To store the predicted labels\n    for i in range(len(testing_values)):\n        predicted.loc[i, \"pred_value\"] = Predict(datapoints[i], decision_tree, debugging) \n        predicted.loc[i, \"pred_probability\"] = Predict(datapoints[i], decision_tree, getProb = 1) \n        \n    return predicted\n\ndebug_actual_labels = list(debug_testing_fold[\"is_open\"]) #For Debugging\ndebug_pred_labels = list(Validation(debug_testing_fold, Debug_Tree)[\"pred_value\"]) \ndebug_pred_probs = list(Validation(debug_testing_fold, Debug_Tree)[\"pred_probability\"])","metadata":{"execution":{"iopub.status.busy":"2021-06-13T05:43:24.673235Z","iopub.execute_input":"2021-06-13T05:43:24.673782Z","iopub.status.idle":"2021-06-13T05:43:24.738626Z","shell.execute_reply.started":"2021-06-13T05:43:24.673739Z","shell.execute_reply":"2021-06-13T05:43:24.736554Z"},"trusted":true},"execution_count":32,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-32-c023d7c533db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mdebug_actual_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdebug_testing_fold\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"is_open\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#For Debugging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mdebug_pred_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mValidation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdebug_testing_fold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDebug_Tree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pred_value\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mdebug_pred_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mValidation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdebug_testing_fold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDebug_Tree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pred_probability\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-32-c023d7c533db>\u001b[0m in \u001b[0;36mValidation\u001b[0;34m(testing_values, decision_tree, debugging)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pred_value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pred_probability\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#To store the predicted labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtesting_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mpredicted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pred_value\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatapoints\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecision_tree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mpredicted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pred_probability\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatapoints\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecision_tree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetProb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-31-26e7b7275c44>\u001b[0m in \u001b[0;36mPredict\u001b[0;34m(datapoint, decision_tree, getProb, debugging)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdatapoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mthreshold\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m                     \u001b[0msub_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecision_tree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"<=\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_tree\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdict\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mgetProb\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mprobability\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecision_tree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"confidence\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"<=\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: '<=78.0'"],"ename":"KeyError","evalue":"'<=78.0'","output_type":"error"}]},{"cell_type":"code","source":"#Classifer Statistics - Confusion Matrix\ndef Confusion_Matrix(actual_labels, predicted_labels) : \n    \n    TP = TN = FP = FN = 0\n    \n    for i, j in zip(actual_labels, predicted_labels) : \n        if i == j and i == 1 :   #True Positive\n            TP += 1\n        elif i == j and i == 0 : #True Negative\n            TN += 1\n        elif i != j and i == 0 : #False Positive \n            FP += 1\n        elif i != j and i == 1 : #False Negative\n            FN += 1\n            \n    return TP, TN, FP, FN\n\ndebug_TP, debug_TN, debug_FP, debug_FN = Confusion_Matrix(debug_actual_labels, debug_pred_labels) #For Debugging\n#print(\"True Positive: \", TP, \"\\tFalse Negative: \", FN)\n#print(\"False Positve: \", FP, \"\\tTrue Negative: \", TN)","metadata":{"execution":{"iopub.status.busy":"2021-06-13T05:43:24.739885Z","iopub.status.idle":"2021-06-13T05:43:24.740694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Confusion Matrix Heat Map\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef CM_Heat_Map(actual_labels, predicted_labels, filename) : \n    \n    TP, TN, FP, FN = Confusion_Matrix(actual_labels, predicted_labels)\n    HM_Value = np.asarray([TP, FN, FP, TN]).reshape(2, 2)\n    x_axis_labels = [\"Predicted Positive\", \"Predicted Negative\"] # labels for x-axis\n    y_axis_labels = [\"Actual Positive\", \"Actual Negative\"]       # labels for y-axis\n    heatmap = sns.heatmap( HM_Value , annot=True, fmt = \".3g\", linewidth = 0.5 , cmap = 'coolwarm', xticklabels=x_axis_labels, yticklabels=y_axis_labels)\n    plt.yticks(rotation = 0) \n    plt.title( \"Confusion Matrix Heat Map\" )\n    plt.savefig(filename + \" CMHM.png\")\n    plt.show()\n\nCM_Heat_Map(debug_actual_labels, debug_pred_labels, \"Debugging\") #For Debugging","metadata":{"execution":{"iopub.status.busy":"2021-06-13T05:43:24.742057Z","iopub.status.idle":"2021-06-13T05:43:24.742822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Classifer Statistics - Accuracy\ndef Metric_Accuracy(TP, TN, FP, FN) : \n    \n    Accuracy = (TP + TN) / (TP + TN + FP + FN)\n    return Accuracy\n\nprint(\"Accuracy: \", Metric_Accuracy(debug_TP, debug_TN, debug_FP, debug_FN) * 100) #For Debugging*","metadata":{"execution":{"iopub.status.busy":"2021-06-13T05:43:24.744088Z","iopub.status.idle":"2021-06-13T05:43:24.744893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Classifer Statistics - Precision, Recall & F-Score\ndef Metric_FMeasure(TP, TN, FP, FN) : \n    \n    Precision = TP / (TP + FP)\n    Recall = TP / (TP + FN)\n    F_Score = (2 * Precision * Recall) / (Precision + Recall)\n    \n    return Precision, Recall, F_Score\n\ndebug_Precision, debug_Recall, debug_F_Score = Metric_FMeasure(debug_TP, debug_TN, debug_FP, debug_FN) #For Debugging\nprint(\"Precision: \", debug_Precision, \"\\tRecall: \", debug_Recall, \"\\tF-Score: \", debug_F_Score)","metadata":{"execution":{"iopub.status.busy":"2021-06-13T05:43:24.746157Z","iopub.status.idle":"2021-06-13T05:43:24.746973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Plotting Graphs\nfrom sklearn import metrics\n\ndef ROC_Curve(actual_labels, predicted_probs, filename) :  \n\n    false_positive_rates, true_positive_rates, thresholds = metrics.roc_curve(actual_labels, predicted_probs, pos_label = 1)\n    plt.figure(dpi=64, figsize=(8,8))\n    plt.plot(false_positive_rates, true_positive_rates, linestyle='-', label='Decision Tree', color = \"red\")\n    plt.plot([0, 1], [0, 1], \"k--\", label='Random guess')\n    plt.xlabel(\"False Positive Rate\")\n    plt.ylabel(\"True Positive Rate\")\n    plt.title(\"ROC Curve\")\n    plt.legend()\n    plt.grid(True)\n    plt.savefig(filename + \" ROCCurve.png\")\n    plt.show()\n    print(\"False Positive Rates: \", false_positive_rates)\n    print(\"True Positive Rates: \", true_positive_rates)\n    \ndef PR_Curve(actual_labels, predicted_probs, filename) :  \n\n    precision_rates, recall_rates, thresholds = metrics.precision_recall_curve(actual_labels, predicted_probs, pos_label = 1)\n    plt.figure(dpi=64, figsize=(8,8))\n    plt.plot(recall_rates, precision_rates, linestyle='-', label='Decision Tree', color = \"red\")\n    plt.plot([0, 1], [0.5, 0.5], \"k--\", label='Random guess')\n    plt.xlabel(\"Recall\")\n    plt.ylabel(\"Precision\")\n    plt.title(\"Precision Recall Curve\")\n    plt.legend()\n    plt.grid(True)\n    plt.savefig(filename + \" PRCurve.png\")\n    plt.show()\n    print(\"Precision Rates: \", precision_rates)\n    print(\"Recall Rates: \", recall_rates)\n\nROC_Curve(debug_actual_labels, debug_pred_probs, \"Debugging\")\nPR_Curve(debug_actual_labels, debug_pred_probs, \"Debugging\")","metadata":{"execution":{"iopub.status.busy":"2021-06-13T05:43:24.748267Z","iopub.status.idle":"2021-06-13T05:43:24.749080Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Training balanced classifier\nbal_training_fold, bal_testing_fold = dataset_split(balanced_dataset)\nbalanced_DTree = C4_5(bal_training_fold, bal_training_fold, bal_training_fold.columns[:-1], max_height = 10)    \n\nbal_actual_labels = list(bal_testing_fold[\"is_open\"]) #Fetch results\nbal_pred_labels = list(Validation(bal_testing_fold, balanced_DTree)[\"pred_value\"]) \nbal_pred_probs = list(Validation(bal_testing_fold, balanced_DTree)[\"pred_probability\"])\n\nbal_TP, bal_TN, bal_FP, bal_FN = Confusion_Matrix(bal_actual_labels, bal_pred_labels) #Fetch metrics\nprint(\"Accuracy : \", Metric_Accuracy(bal_TP, bal_TN, bal_FP, bal_FN))\n\nCM_Heat_Map(bal_actual_labels, bal_pred_labels, \"Balanced Set\") #Fetch graphs\nROC_Curve(bal_actual_labels, bal_pred_probs, \"Balanced Set\") ","metadata":{"execution":{"iopub.status.busy":"2021-06-13T05:43:24.749977Z","iopub.status.idle":"2021-06-13T05:43:24.750553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Training imbalanced classifier\nimbal_training_fold, imbal_testing_fold = dataset_split(flipClass(imbalanced_dataset))\nimbalanced_DTree = C4_5(imbal_training_fold, imbal_training_fold, imbal_training_fold.columns[:-1], max_height = 10)    \n\nimbal_actual_labels = list(imbal_testing_fold[\"is_open\"]) #Fetch results\nimbal_pred_labels = list(Validation(imbal_testing_fold, imbalanced_DTree)[\"pred_value\"]) \nimbal_pred_probs = list(Validation(imbal_testing_fold, imbalanced_DTree)[\"pred_probability\"])\n\nimbal_TP, imbal_TN, imbal_FP, imbal_FN = Confusion_Matrix(imbal_actual_labels, imbal_pred_labels) #Fetch metrics\nprint(\"F-Measure : \", Metric_FMeasure(imbal_TP, imbal_TN, imbal_FP, imbal_FN))\n\nCM_Heat_Map(imbal_actual_labels, imbal_pred_labels, \"Imbalanced Set\") #Fetch graphs\nPR_Curve(imbal_actual_labels, imbal_pred_probs, \"Imbalanced Set\") ","metadata":{"execution":{"iopub.status.busy":"2021-06-13T05:43:24.751437Z","iopub.status.idle":"2021-06-13T05:43:24.752030Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#K-fold cross validation - Fetching sound metrics\nfrom sklearn.model_selection import StratifiedKFold\n\ndef KFold_Cross_Validation(dataset, method = \"balanced\", max_height = 10) : \n    \n    values = []\n    KFModel = StratifiedKFold(n_splits = 5, random_state = 100, shuffle = True)\n    for training_index, testing_index in KFModel.split(dataset, dataset[\"is_open\"]) :\n\n        kfold_training_values, kfold_testing_values = dataset.loc[training_index, : ], dataset.loc[testing_index, : ] \n        Kfold_DTree = C4_5(kfold_training_values, kfold_training_values, kfold_training_values.columns[:-1], max_height = max_height)\n    \n        kfold_actual_labels = list(kfold_testing_values[\"is_open\"]) \n        kfold_pred_labels = list(Validation(kfold_testing_values, Kfold_DTree)[\"pred_value\"])\n        kfold_TP, kfold_TN, kfold_FP, kfold_FN = Confusion_Matrix(kfold_actual_labels, kfold_pred_labels)\n        \n        if method == \"balanced\" : \n            values.append(Metric_Accuracy(kfold_TP, kfold_TN, kfold_FP, kfold_FN))\n        else :\n            values.append(Metric_FMeasure(kfold_TP, kfold_TN, kfold_FP, kfold_FN)[2]) \n    \n    return values","metadata":{"execution":{"iopub.status.busy":"2021-06-13T05:43:24.752920Z","iopub.status.idle":"2021-06-13T05:43:24.753469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Error Plot\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndef Error_Bars(values, metric, filename) : \n\n    Mean = np.mean(values)\n    STD = np.std(values)\n    \n    #Plot Metrics\n    values = [Mean]\n    errors = [STD]\n    labels = [\"Decision Tree\"]\n    x_pos = np.arange(len(labels))\n\n    # Build the plot\n    fig, ax = plt.subplots(dpi=64, figsize = (5, 5))\n    ax.bar(x_pos, values, yerr=errors, align='center', alpha=0.5, ecolor='black', capsize=10)\n    ax.set_ylabel(metric)\n    ax.set_xticks(x_pos)\n    ax.set_xticklabels(labels)\n    ax.set_title(\"Metrics of Decision Tree\")\n    ax.yaxis.grid(True)\n    plt.ylim([0.0, 1.0])\n    plt.tight_layout()\n    plt.savefig(filename + \".png\")\n    plt.show()\n    \n    print(metric, \" : %.3f\" %Mean, \" STD : %.3f\" %STD)","metadata":{"execution":{"iopub.status.busy":"2021-06-13T05:43:24.754314Z","iopub.status.idle":"2021-06-13T05:43:24.754831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Fetching Balanced K-Fold Metrics\naccuracy_values = KFold_Cross_Validation(balanced_dataset, method = \"balanced\", max_height = 10)\nprint(\"Accuracy Values: \", accuracy_values)\nError_Bars(accuracy_values, \"Accuracy\", \"Balanced Accuracy Bar Plot\") #Fetch Graphs","metadata":{"execution":{"iopub.status.busy":"2021-06-13T05:43:24.755690Z","iopub.status.idle":"2021-06-13T05:43:24.756254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Fetching Imbalanced K-Fold Metrics\nfmeasure_values = KFold_Cross_Validation(imbalanced_dataset, method = \"imbalanced\", max_height = 10)\nprint(\"F-Measure Values: \", fmeasure_values)\nError_Bars(fmeasure_values, \"F-Measure\", \"Imbalanced F-Measure Bar Plot\") #Fetch Graphs","metadata":{"execution":{"iopub.status.busy":"2021-06-13T05:43:24.757161Z","iopub.status.idle":"2021-06-13T05:43:24.757708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#References: \n#https://www.python-course.eu/Decision_Trees.php\n#https://anderfernandez.com/en/blog/code-decision-tree-python-from-scratch/\n#https://www.analyticssteps.com/blogs/what-gini-index-and-information-gain-decision-trees\n#https://anderfernandez.com/en/blog/code-decision-tree-python-from-scratch/\n\n#https://sefiks.com/2018/05/13/a-step-by-step-c4-5-decision-tree-example/\n#https://sefiks.com/2017/11/20/a-step-by-step-id3-decision-tree-example/\n#https://cis.temple.edu/~giorgio/cis587/readings/id3-c45.html\n\n#https://support.minitab.com/en-us/minitab-express/1/help-and-how-to/modeling-statistics/regression/supporting-topics/basics/what-are-categorical-discrete-and-continuous-variables/\n#https://machinewithdata.com/2018/07/10/how-to-calculate-gain-ratio/#:~:text=Gain%20ratio%20overcomes%20the%20problem,add%20penalty%20to%20information%20gain.\n#https://www.kdnuggets.com/2020/01/decision-tree-algorithm-explained.html","metadata":{"execution":{"iopub.status.busy":"2021-06-13T05:43:24.758547Z","iopub.status.idle":"2021-06-13T05:43:24.759107Z"},"trusted":true},"execution_count":null,"outputs":[]}]}